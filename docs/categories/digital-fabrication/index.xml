<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Digital Fabrication on Dan Oved&#39;s blog</title>
    <link>https://oveddan.github.io/blog/categories/digital-fabrication/</link>
    <description>Recent content in Digital Fabrication on Dan Oved&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 13 Nov 2017 22:21:18 -0500</lastBuildDate>
    
	<atom:link href="https://oveddan.github.io/blog/categories/digital-fabrication/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Predicting Gaze in Python with the Eye Tracking for Everyone Model</title>
      <link>https://oveddan.github.io/blog/posts/gaze/predicting-gaze-with-the-model/</link>
      <pubDate>Mon, 13 Nov 2017 22:21:18 -0500</pubDate>
      
      <guid>https://oveddan.github.io/blog/posts/gaze/predicting-gaze-with-the-model/</guid>
      <description>View the ipython notebook with the code here
My next goal was to try to get eye gaze prediction working using the Eye Tracking for Everyone model. I wanted to get it working in Python so that it would be platform agnostic; the idea would be to eventually get this working on any computer with a webcam and decent GPU. The ultimate deployment would be on the NVidia Jetson TX2, a single-board computer with a powerful GPU and support for six camera inputs.</description>
    </item>
    
    <item>
      <title>The Gaze Project - Play Testing</title>
      <link>https://oveddan.github.io/blog/posts/gaze/play-testing/</link>
      <pubDate>Tue, 07 Nov 2017 20:16:51 -0500</pubDate>
      
      <guid>https://oveddan.github.io/blog/posts/gaze/play-testing/</guid>
      <description>For Intro to Physical Computing this week, we were to play test our final project with our classmates.
For the Gaze Project, I set up the following for the test:
Assumptions  Initially - people will try to follow the animation with their eyes, but they will notice that doing that causes it to go away. They will eventually learn to focus on one point and see the animation grow with their peripherals.</description>
    </item>
    
    <item>
      <title>The Gaze Project - Proposal</title>
      <link>https://oveddan.github.io/blog/posts/gaze/proposal/</link>
      <pubDate>Tue, 31 Oct 2017 22:55:41 -0400</pubDate>
      
      <guid>https://oveddan.github.io/blog/posts/gaze/proposal/</guid>
      <description>This is the proposal for the Gaze Project, an art piece that is activated the longer and the more people gaze directly at it. It would encourage being present and engaged, and connect the fellow participants with each other over moments of mindfulness. This will be my final project for Intro to Physical Computing and Design for Digital Fabrication.
I will use small cameras and computer vision to detect the quantify of people present and how many of them are gazing at a specific point, and reveal parts of the piece using motors and other physical controls.</description>
    </item>
    
    <item>
      <title>From Shader to Physical Animation - the Voronoi Cellular Zoetrope</title>
      <link>https://oveddan.github.io/blog/posts/design_for_digital_fabrication/voronoi-cellular-zoetrope/</link>
      <pubDate>Wed, 25 Oct 2017 16:10:23 -0400</pubDate>
      
      <guid>https://oveddan.github.io/blog/posts/design_for_digital_fabrication/voronoi-cellular-zoetrope/</guid>
      <description>This is a continuation of Part 1 and Part 2 of the Zeotrope project.
Motivation This was the final phase of our homework assignment for Design for Digital Fabrication which was to design something algorithmically and laser cut it. Now that the mechanics of the Zoetrope animation were proven out the previous week, it was time to generate something elaborate. I was particularly inspired by John Edmark&amp;rsquo;s animated bloom sculptures:   After seeing what he made, my goal was to create an animation that:</description>
    </item>
    
    <item>
      <title>Generating Zoetrope Animations</title>
      <link>https://oveddan.github.io/blog/posts/design_for_digital_fabrication/generating-zoetrope-animations/</link>
      <pubDate>Thu, 12 Oct 2017 13:25:28 -0400</pubDate>
      
      <guid>https://oveddan.github.io/blog/posts/design_for_digital_fabrication/generating-zoetrope-animations/</guid>
      <description>This is a continuation of Part 1 of the Zeotrope project for my Design for Digital Fabrication class. The final phase can be seen in Part 3
The next step of the project was to prototype the mechanics of the animation. The main questions were:
 How many frames of animation are optimal? What should the rotation speed be? At what rate should the lights flash?  The Rotation Mechanism - a Turntable There were many examples of Zoetropes on the web - the majority of them used a turntable to rotate the animation.</description>
    </item>
    
  </channel>
</rss>