<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Physical Computing on Dan Oved&#39;s Blog</title>
    <link>https://oveddan.github.io/blog/tags/physical-computing/</link>
    <description>Recent content in Physical Computing on Dan Oved&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 13 Nov 2017 22:21:18 -0500</lastBuildDate>
    
	<atom:link href="https://oveddan.github.io/blog/tags/physical-computing/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Predicting Gaze in Python with the Eye Tracking for Everyone Model</title>
      <link>https://oveddan.github.io/blog/posts/gaze/predicting-gaze-with-the-model/</link>
      <pubDate>Mon, 13 Nov 2017 22:21:18 -0500</pubDate>
      
      <guid>https://oveddan.github.io/blog/posts/gaze/predicting-gaze-with-the-model/</guid>
      <description>View the ipython notebook with the code here
My next goal was to try to get eye gaze prediction working using the Eye Tracking for Everyone model. I wanted to get it working in Python so that it would be platform agnostic; the idea would be to eventually get this working on any computer with a webcam and decent GPU. The ultimate deployment would be on the NVidia Jetson TX2, a single-board computer with a powerful GPU and support for six camera inputs.</description>
    </item>
    
    <item>
      <title>The Gaze Project - Concepts and Play Testing</title>
      <link>https://oveddan.github.io/blog/posts/gaze/concepts-and-play-testing/</link>
      <pubDate>Tue, 07 Nov 2017 20:16:51 -0500</pubDate>
      
      <guid>https://oveddan.github.io/blog/posts/gaze/concepts-and-play-testing/</guid>
      <description>For Intro to Physical Computing this week, we were to play test our final project with our classmates.
I collaborated with Katya Rozanova to come up with initial designs.
Here are some of her sketches and concepts:
We ultimately settled on a form with vertical columns that rotate and reveal certain patterns depending on where you are looking. We chose this because it would be easier to build; with a single axis, the x-axis, we could have servos rotate the columns to represent the gaze is on that axis.</description>
    </item>
    
    <item>
      <title>The Gaze Project - Proposal</title>
      <link>https://oveddan.github.io/blog/posts/gaze/proposal/</link>
      <pubDate>Tue, 31 Oct 2017 22:55:41 -0400</pubDate>
      
      <guid>https://oveddan.github.io/blog/posts/gaze/proposal/</guid>
      <description>This is the proposal for the Gaze Project, an art piece that is activated the longer and the more people gaze directly at it. It would encourage being present and engaged, and connect the fellow participants with each other over moments of mindfulness. This will be my final project for Intro to Physical Computing and Design for Digital Fabrication.
I will use small cameras and computer vision to detect the quantify of people present and how many of them are gazing at a specific point, and reveal parts of the piece using motors and other physical controls.</description>
    </item>
    
    <item>
      <title>Weather Diorama</title>
      <link>https://oveddan.github.io/blog/posts/physical_computing/weather-diorama/</link>
      <pubDate>Sun, 29 Oct 2017 20:33:47 -0400</pubDate>
      
      <guid>https://oveddan.github.io/blog/posts/physical_computing/weather-diorama/</guid>
      <description>Background / Motivation This was my teams&amp;rsquo; project for our Intro to Physical Computing midterm, where we were encouraged to use serial communication. Barak Chamo and Mai Arakida Izsak were the other team members, and contributed to this post.
After seeing Barak create his initial Fuji Diorama:
We thought it would be interesting to insert lights between the layers that could change the appearance of the scene depend on the time of day or weather.</description>
    </item>
    
    <item>
      <title>Generating Zoetrope Animations</title>
      <link>https://oveddan.github.io/blog/posts/design_for_digital_fabrication/generating-zoetrope-animations/</link>
      <pubDate>Thu, 12 Oct 2017 13:25:28 -0400</pubDate>
      
      <guid>https://oveddan.github.io/blog/posts/design_for_digital_fabrication/generating-zoetrope-animations/</guid>
      <description>This is a continuation of Part 1 of the Zeotrope project for my Design for Digital Fabrication class. The final phase can be seen in Part 3
The next step of the project was to prototype the mechanics of the animation. The main questions were:
 How many frames of animation are optimal? What should the rotation speed be? At what rate should the lights flash?  The Rotation Mechanism - a Turntable There were many examples of Zoetropes on the web - the majority of them used a turntable to rotate the animation.</description>
    </item>
    
    <item>
      <title>Light Loop</title>
      <link>https://oveddan.github.io/blog/posts/physical_computing/light-loop/</link>
      <pubDate>Tue, 26 Sep 2017 00:00:00 -0400</pubDate>
      
      <guid>https://oveddan.github.io/blog/posts/physical_computing/light-loop/</guid>
      <description>The Features The goal of this project is to use analog input and digital input to drive digital output.
This is accomplished by detecting when a photoresistor, an analog input, gets a certain amount of light. When that happens, it triggers an animation on the opposite end of the LEDS which travels through them back towards the photoresistor. When the photoresistor picks up enough light from the animation that has reached it, it triggers the start of the animation again.</description>
    </item>
    
    <item>
      <title>Doubling the Battery Power</title>
      <link>https://oveddan.github.io/blog/posts/physical_computing/batteries-in-parallel/</link>
      <pubDate>Thu, 21 Sep 2017 00:00:00 -0400</pubDate>
      
      <guid>https://oveddan.github.io/blog/posts/physical_computing/batteries-in-parallel/</guid>
      <description>A project I worked on this past summer required a portable power source of 3.3 - 4 volts. I used the Tenergy Li-Ion 3.7v 2600 mAH rechargeable battery, connected to the Adafruit LiPoly backpack charger and a Teensy 3.2. In this system, the battery would be drained after only about half an hour; I wanted to increase the capacity but did not know how.
Then last week, for my Basic Analog Circuit course, I learned about connecting multiple batteries in parallel to maintain the same voltage but increase the current and capacity by connecting the powers and grounds together of the batteries.</description>
    </item>
    
    <item>
      <title>What Is Physical Interaction?</title>
      <link>https://oveddan.github.io/blog/posts/physical_computing/what-is-physical-interaction/</link>
      <pubDate>Thu, 07 Sep 2017 00:00:00 -0400</pubDate>
      
      <guid>https://oveddan.github.io/blog/posts/physical_computing/what-is-physical-interaction/</guid>
      <description>According to Chris Crawford in The Art of Interactive Design, Physical interaction is where a human interfaces with another entity, such as a person or computer, and that entity responds based on that action. It must be two-way; uni-directional communication is not interaction.
Examples of physical interaction:
 Typing in a text editor and seeing letters appearing. Painting with hand-help controllers in virtual reality. Having a conversation with another person. Playing a competitive sport.</description>
    </item>
    
  </channel>
</rss>