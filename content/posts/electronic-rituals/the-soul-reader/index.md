---
title: 'The Soul Reader'
date: 2018-05-01T22:05:18-04:00
tags: ["generative", "randomness", "computer vision", "AI", "subconscious"]
categories: [""]
classes: [""]
draft: false
---

The Soul Reader is my final for [Electronic Rituals, Oracles, and Storytelling](ToDo: link).  It is a generative system for visuals that is driven by a viewer's gaze and subconscious.

{{< vimeo 267715929 >}}

It is a screen based experience which uses the focus of a viewer's gaze to continuously generate imagery. It shows the viewer different colors, text, shapes, and scenes, and uses recent advances in machine learning to determine which of these elements the viewer's gaze is focused on. It uses this information to continuously generating new patterns, colors, and shapes similar to what the viewer was fixated on, and renders them away from the center of the gaze. This way, the scene is constantly changing out of the field of view.

Itâ€™s like a choose your own adventure driven by your subconscious with no ending. It is both continuous and infinite. One person who continues where the another left off will start with the imagery generated by that other person, resulting in an experience shaped by all the people that used it before.

Although I have implemented the same gaze detection platform in the past, in it's current state, it's not yet integrated into the application;  the cursor is used until that will work, which is soon.  Scroll down for an explaination of the difficulties in getting the integration right.

## Background

I became interested in gaze tracking first through discovering the research paper Eye Tracking for Everyone, which allowed for eye gaze tracking with just a webcam and a neural network. I used the pre-trained model from this research paper for my installation [Presence](http://www.danioved.com/portfolio/presence/) at the last ITP Winter Show, which was a kinetic sculpture that reacted to a users gaze.

{{< vimeo 259512916 >}}

After realizing that the technology works, I got excited about the possiblities and this and it lead me on a search for how it can be used; I discovered that it has been extensively researched, and there plenty of insight into how it connects to the psyche, the mind, and commumication.  I realized that there is a connection between the gaze and the subconcious, and it would be interesting to create an experience that explores this concept.

## Relevant Research

Narrow field of view of the eye

How eye scans a scene.

Eyes look up when challenged.

## How The Experience Works

The application is built in C++ OpenFrameworks. This was chosen because I wanted to be able to convey the stream of concious aspect of the gaze by blending different rendered elements together; OpenFrameworks makes this easy by giving access to computer graphics shaders and providing a framework to render them into each other via buffers.

**The OpenFrameworks code can be seen [here](https://github.com/oveddan/the-soul-reader/tree/master/src), and the shader that does the focus blur [here](https://github.com/oveddan/the-soul-reader/blob/master/bin/data/shaders/focusBlur.frag).**

The experience starts with a few random bits of text and colors shown to the viewer.  When the viewer focuses on something, a ripple effect begins to appear around the focal point.  After a period of time, around 600 ms, which is the time researches have concluded it takes to scan a picture, the application determines that the this particular area of focus has drawn the attention of the user.  It then generates similar shapes or text away from where the center of focus is.

### The Color System

For the color of new elements, the Hue Saturation and Brightness system is used because it provides a computational method for picking similar or complimentary colors.  

{{< image src="images/hsb_wheel" caption="THe HSB system is used to find similar colors.  Both the hue and the brightness values are modified by a small, random amount to get a similar color. Modifying the hue goes around the wheel, while modifying the saturation goes outside from inside of the wheel." >}}

The first sets of colors that are chosen are complimentary to each other, which allows for the viewer to follow the different paths that each color would take them.   With the HSB system, these compliments are easily calculatable by evenly moving around the color wheel in a proportion to the number of complimentary colors:

```c++
int randomHue = std::round(ofRandom(0, 255));
int randomSaturation =std::round(ofRandom(100, 255));
ofColor baseColor = ofColor::fromHsb(randomHue, randomSaturation, 255);

vector<ColorElement> elements;
int numberOfColors = 2;
// there are 255 possible values for hue.
int changePerColor = 255 / numberOfColors;

for(int i = 0; i < numberOfColors; i++) {
    int newHue = randomHue + changePerColor * i;
    // move back to the start of the wheel
    if (newHue > 255) { newHue = newHue - 255; }
    ofColor color = ofColor::fromHsb(newHue, randomSaturation, 255);
    
    // get random geometry for element
    int randomWidth = ofRandom(0, ofGetWidth() / 4);
    int randomHeight = ofRandom(0, ofGetHeight() / 4);
    int randomX = ofRandom(0, ofGetWidth());
    int randomY =  ofRandom(0, ofGetHeight());
    
    ColorElement element(color, randomX, randomY, randomWidth, randomHeight);
    
    elements.push_back(element);
}
```

When a new element is created, its colors are generated by changing the hue and saturation values of the focused on element by a random value:

```c++
ofColor getSimilarColor(ofColor color) {
    float newSaturation = round(ofClamp(ofRandom(-20, 20) + color.getSaturation(), 0., 255.));
    float newHue = round(ofClamp(ofRandom(-20, 20) + color.getHue(), 0., 255.));
    float newBrightness = color.getBrightness();
    
    return ofColor::fromHsb(newHue, newSaturation, newBrightness);
}
```

This produces similar colors:

{{< image src="images/similar_colors" caption="Words in the same region of the Wordnet synonym graph.">}}

### The Text

When a user focuses on a piece of text, another word is rendered away from area of focus.  The system almost always chooses the word to be similar to the currently focused on word.  To find similar words, Princeton's Wordnet in json form is used to look up words that are in the same family along a graph.  This allows different paths of words to be followed.  Occasionally, a random word is chosen instead of one within the family, to avoid being infinitely stuck in a loop with words families that don't branch out.

```c++
float probabilityDifferentWord = 0.1;

bool shouldGetSomethingDifferent() {
    float randomValue = ofRandom(0., 1.);
    
    return randomValue <= probabilityDifferentWord;
}

void WordElement::loadWord() {
    font.load("fonts/NewsCycle-Bold.ttf", 40);
    
    if (synsetKey == "" || shouldGetSomethingDifferent()) {
        loadRandomWord();
    } else {
        loadSimilarWord();
    }
}

```

View the code in its entirery [here.](https://github.com/oveddan/the-soul-reader/blob/e3955f71b126d5860bb586147bd4cf85be792963/src/element.cpp#L154)


The OpenFrameworks app requests a similar word from a python app that looks the words in the Wordnet json file.  This word part of the system is written in python because of its ease with working with json data, which is a pain in c++.  It is hosted in a flask server, and the OpenFrameworks app requests the words via get requests.  

```python
import json
import random

# load wordnet json
words_data=json.load(open('./wordnet.json'))

# load the synonyms.
synset = words_data['synset']
synset_keys = list(synset.keys())
num_words = len(synset_keys)

def random_word_key():
  random_key_index = random.randint(0, num_words)
  return synset_keys[random_key_index]

def get_word(key):
  return synset[key]

def get_random_word_key(key):
  words_for_key = get_word(key)['word']
  return random.choice(words_for_key)

# convert into a key:word which is parceable in c++
def to_key_word(key, word):
  return key + ':'+word.replace('_', ' ')

def random_word():
  key = random_word_key()
  word = get_random_word_key(key)
  return to_key_word(key, word)

def similar_word(word_key):
  word = synset[word_key]

  pointers = word['pointer']
  random_pointer = random.choice(pointers)
  key = random_pointer['synset']
  word = get_random_word_key(key)

  return to_key_word(key, word)
 ```

 This produces words that are for the most part similar:

{{< fullsizeimage src="images/similar_words" >}}

## How does this fit into the class?

* Randomness - way it can be done is interesting
* Artificial Intelligence interpretation of you, (and semi inaccurate)

## Gaze Estimation Implementation

{{< fullsizeimage src="images/system-diagram" >}}

 I still have not figured out the OpenFrameworks to video stream, but it seems the way to go is to have runway do it and OF grabs the feed from there.  

On my last go around with gaze detection, some of the biggest feedback I got was that they wanted it be very responsive to their gaze.  Getting real-time performance here is key.

The way I set it up on the python side is that both the opencv feature extraction and the feed forwarding of the neural network happen concurrently using multithreading.  This means that both the faces are being detected and the neural network is feeding forward at the same time.  When the neural network is done being fed forward, there are already features from the next frame ready to be fed forward through the network.


**Aa collaboratory notebook with the gaze detection mechanism can be seen [here](https://colab.research.google.com/drive/11s5IQkI8H-kIn00Kg6Sqp-dD3RwsICdE).**

The problem with the existing setup was:
* required gaming desktop
* required linux to be installed with cuda
* if running over a long period, consumes a lot of power.
* eyes were not detectable with glasses


Wanted something real time, more portable, and long lasting.
Secondary goal accessible to other environments.

First idea was to convert tensorflow, and then tensorflow.js.

Could not convert to tensorflow with the converter (show the error)

Then tried mxdnn to first convert to global format.

Had this issue.

### Jetson

Then decided to give Jetson a shot again.  Got the system working
by installing the Jetpack

Downloaded jetson-inference, was able to run their examples well.

Show pedestran detection

Show face detection.

Challenge was I had to use C++ with TensorRT, since it's pretty much impossible
to install anything but what comes with the jetback.  TensorRT is not really documented well.

Anyways started working on C++ system.

biggest pain was everything was void* pointers! sometimes (void**)&

First used dlib to do face and eye detection.  Results were very accurate, with outlines drawn on the face and eye glasses.

Then converted it to create boxes around face and eyes.  This even worked with glasses:

Then had to crop and scale images to feed into the neural network.  It takes 224x224.

Wrote operators in cuda to do cropping.

Wrote an operator to generate a 25x25 face grid.

Figured out how to render those things to the buffer.  This was a huge pain (took about a day).

Show results cropped and scaled to 224.

Then got a gaze feeding through the network.

Rendered red dot output.

Mapped to screen coordinates using logic from before.  It didn't work.

Wanted to then use open frameworks.

Spent a long time trying to get it to install on Jetson.  Koji had a working on but I could not reproduce.

Met with Kyle.  He asked me what is this for? I told him an installation.  He told me the only reason to use a Jetson is for when it needs to be run on a battery, like on a drone or robot.  he said if I have somethign working I should just use what I have and focus on the experience.  

Also said it was better to not have a big behemoth.

In the end I decided to get the original solution working. It was in python. I still didn't want to have to bring computer
Spoke to Cristobal Venezuela who has Runway.  He said you can deploy it in the cloud and connect to it over ethernet and it's really fast.

I decided to try the runway approach, but with the camera feed happening from the client.  I would use zeromq to communicate between the openframeworks client and the python server.

Zeromq was documented very well.  I was able to setup a docker container with the server running, and a pythong client using zeromq.  I used a full-duplex communication protocol.  This worked on my computer.

Then it came time to installed it in my xcode.  I could for the life of me not get zeromq to work.

(Show Screenshot)

Then it was time to move onto the UI. I would work on it later.


